{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load toxigen train dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 250,951 training examples\n",
    "TG_data = load_dataset(\"skg/toxigen-data\", name=\"train\", use_auth_token='') # You need use_auth_token for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there exist 17 samples among 250,951 machine-generated statements with the prompt value 'prompt' in the original dataset, \n",
    "# we remove such 17 samples.\n",
    "skip_idx_list = []\n",
    "for i, one_sample in enumerate(TG_data['train']):\n",
    "    if one_sample['prompt'] == 'prompt':\n",
    "        skip_idx_list.append(i)\n",
    "\n",
    "# 250,934 samples\n",
    "TG_data['train'] = TG_data['train'].select(\n",
    "    (\n",
    "        i for i in range(len(TG_data['train'])) \n",
    "        if i not in set(skip_idx_list)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the process was conducted successfully\n",
    "for one_sample in TG_data['train']:\n",
    "    assert one_sample['prompt'] != 'prompt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for anonymization of private information such as email address, urls, and user or channel mention\n",
    "# We follow the implementation in https://github.com/dhfbk/hate-speech-artifacts.\n",
    "\n",
    "import re\n",
    "from html import unescape\n",
    "import wordsegment as ws\n",
    "ws.load()   # load the vocabulary for wordsegment\n",
    "\n",
    "def clean_text(example):\n",
    "    def regex_match_segmentation(match):\n",
    "        # Useful to segment hashtags found via regexes\n",
    "        return ' '.join(ws.segment(match.group(0)))\n",
    "    \n",
    "    text = example['generation']\n",
    "    text = unescape(text)   # HTML tags handling\n",
    "    text = text.lower()     # make it lowercase\n",
    "\n",
    "    # Normalize most common space-split URLs (for noisy Stormfront data)\n",
    "    text = text.replace(\"http : //\", \"http://\")\n",
    "    text = text.replace(\"https : //\", \"https://\")\n",
    "    \n",
    "    # Replace email addresses\n",
    "    text = re.sub(r\"(?i)\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b\", \"[EMAIL]\", text)\n",
    "\n",
    "    # Replace URLs\n",
    "    # based on https://github.com/dongpng/cad_naacl2021/blob/main/src/contextual_abuse_dataset.py\n",
    "    text = re.sub(r\"\\[([^\\[\\]]+)\\]\\((https:\\/\\/(.*?))\\)\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\[([^\\[\\]]+)\\]\\((\\/message\\/compose(.*?))\\)\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\[([^\\[\\]]+)\\]\\((\\/r\\/(.*?))\\)\", r\"\\1\", text)\n",
    "    text = re.sub(r'http(s?):\\/\\/[^\\r\\n\\t\\f\\v )\\]\\}]+', '[URL]', text) # excludes trailing parentheses too\n",
    "    text = re.sub(r'www\\.\\S+', '[URL]', text)\n",
    "    \n",
    "    # Replace user/channel mentions\n",
    "    text = re.sub(r\"\\/u\\/\\w+\", \"[USER]\", text) # /u/user on Reddit only\n",
    "    text = re.sub(r\"\\/r\\/\\w+\", \"[USER]\", text) # /r/subreddit on Reddit only\n",
    "    text = re.sub(r\"@[A-Za-z0-9_-]+\", \"[USER]\", text) # @user on Twitter and Gab only\n",
    "\n",
    "    # Segment hashtags, and clean newlines and tabs\n",
    "    text = re.sub(r\"#[A-Za-z0-9]+\", regex_match_segmentation, text)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\"[linebreak]\", \" \") # newlines as in Cad data\n",
    "\n",
    "    # Replace non-standard characters with simple space\n",
    "    text = text.replace(u'\\xa0', u' ')    # no-break space\n",
    "    text = text.replace(u'\\u200d', u' ')  # zero-width joiner\n",
    "    \n",
    "    example['generation'] = text.strip()\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anonymize private information such as email address, urls, and user or channel mention\n",
    "total_dataset = TG_data['train'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_example_list (a list of unique examples used in prompt)\n",
    "temp_example_list = []\n",
    "for i, one_sample in enumerate(total_dataset):\n",
    "    temp_example_list += [x[2:] for x in one_sample['prompt'].split('\\\\n')[:-1]]\n",
    "\n",
    "# 522 unique examples\n",
    "unique_example_list = sorted(list(set(temp_example_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_example2index (example -> index)\n",
    "EXAMPLE_BASE_NUM = 1000\n",
    "unique_example2index = dict()\n",
    "for i, one_single_prompt in enumerate(unique_example_list):\n",
    "    unique_example2index[one_single_prompt] = i + EXAMPLE_BASE_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23322 prompts (sets of examples)\n",
    "unique_prompt_list = sorted(list(set(total_dataset['prompt'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt2index (prompt -> index)\n",
    "# prompt_index2example_index_list (index of prompt -> indexes of examples (a list))\n",
    "PROMPT_BASE_NUM = 100000\n",
    "prompt2index = dict()\n",
    "prompt_index2example_index_list = dict()\n",
    "for i, one_whole_prompt in enumerate(unique_prompt_list):\n",
    "    prompt2index[one_whole_prompt] = i + PROMPT_BASE_NUM\n",
    "    prompt_index2example_index_list[i + PROMPT_BASE_NUM] = sorted([unique_example2index[x[2:]] for x in one_whole_prompt.split('\\\\n')[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# total file construction\n",
    "# csv with sent0 and sent1\n",
    "temp_list = []\n",
    "for i, one_sample in enumerate(total_dataset):\n",
    "    # sent0_label (the index of prompt), sent0(machine-generated statement)\n",
    "    temp_list.append([prompt2index[one_sample['prompt']], one_sample['generation']]) \n",
    "\n",
    "for i, one_sample in enumerate(total_dataset):\n",
    "    candidate_pos_list = [x[2:] for x in one_sample['prompt'].split('\\\\n')[:-1]]\n",
    "    assert len(candidate_pos_list) > 0\n",
    "    selected_pos_prompt = random.choice(candidate_pos_list)\n",
    "    # sent1_label (the index of example as a postivie)\n",
    "    temp_list[i].append(unique_example2index[selected_pos_prompt]) \n",
    "    # sent1 (the example as a positive)\n",
    "    temp_list[i].append(selected_pos_prompt) \n",
    "df = pd.DataFrame(temp_list, columns=['sent0_label', 'sent0', 'sent1_label', 'sent1'])\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv('data/conprompt_pre-train_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/prompt_index2example_index_list.pickle','wb') as fw:\n",
    "    pickle.dump(prompt_index2example_index_list, fw)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
